### kNN-k近邻算法
- 欧拉距离
- kNN的过程
```
from math import sqrt
distances = []
for x_train in X_train:
    d = sqrt(np.sum( (x_train - x)**2))
    distances.append(d)
```
- 给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，基于k个“邻居”进行预测
- k近邻法的三个基本要素
   1. k的选择
   2. 距离度量
   3. 分类决策规则
- 可以使用k近邻解决回归问题
- 维数灾难

### 线性回归法
- 解决回归问题
- 思想简单 实现容易
- 许多强大的非线性模型的基础
- 结果具有很好的可解释性
- 蕴含机器学习中的很多重要思想

### 回归算法的评价
- 均方误差(MSE)
- 均方根误差(RMSE)
- 平均绝对误差(MAE)

### 逻辑回归(LogisticRegression)
- 逻辑回归假设数据服从**伯努利分布**，通过**极大化似然函数**的方法，运用**梯度下降**来求解参数，达到将数据二分类的目的
- 逻辑回归的基本假设
- 逻辑回归的损失函数
- 逻辑回归的求解方法
   1. 批梯度下降：获得全局最优解。缺点是在更新每个参数的时候需要遍历所有数据，计算量大，有很多冗余计算
   2. 随机梯度下降：随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
   3. 小批量梯度下降：小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果
- 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？
   1. 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。这个式子的更新速度只和xij，yi相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。